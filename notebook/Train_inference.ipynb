{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import wget\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from algorithms.Networks_pytorch import *\n",
    "from algorithms.Dataset_manipulation import *\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: still a lot of work to do here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config\n",
    "\n",
    "Make sure to change these configs before running the whole notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = 'ST4000DM000'\n",
    "# here you can select the model. This is the one tested.\n",
    "model = 'ST3000DM001'\n",
    "#years = ['2016', '2017', '2018']\n",
    "years = ['2014', '2015', '2016', '2017', '2018']\n",
    "# many parameters that could be changed, both for unbalancing, for networks and for features.\n",
    "windowing = 1\n",
    "min_days_HDD = 115\n",
    "# TODO: Can be adjusted by dynamic parameters\n",
    "days_considered_as_failure = 7\n",
    "test_train_perc = 0.3\n",
    "# type of oversampling\n",
    "oversample_undersample = 2\n",
    "# balancing factor (major/minor = balancing_normal_failed)\n",
    "# TODO: We can calculate the imbalance ratio of the dataset and use this ratio to adjust the balancing factor.\n",
    "balancing_normal_failed = 20\n",
    "history_signal = 32\n",
    "# type of classifier\n",
    "classifier = 'LSTM'\n",
    "# if you extract features for RF for example. Not tested\n",
    "perform_features_extraction = False\n",
    "CUDA_DEV = \"0\"\n",
    "# if automatically select best features\n",
    "ranking = 'Ok'\n",
    "num_features = 18\n",
    "overlap = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomForestClassification(X_train, Y_train, X_test, Y_test, metric, **args):\n",
    "    Y_test_real = []\n",
    "    prediction = []\n",
    "    # Train and validate the network using RandomForest\n",
    "    X_train, Y_train = shuffle(X_train, Y_train)\n",
    "    model = RandomForestClassifier(n_estimators=30, min_samples_split=10, random_state=3)\n",
    "    model.fit(X_train[:, :], Y_train)\n",
    "    prediction = model.predict(X_test)\n",
    "    Y_test_real = Y_test\n",
    "    report_metrics(Y_test_real, prediction, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TCNClassification(X_train, Y_train, X_test, Y_test, metric, **args):\n",
    "    # Train and validate the network using TCN\n",
    "    net_train_validate_tcn(args['net'], args['optimizer'], X_train, Y_train, X_test, Y_test, args['epochs'], args['batch_size'], args['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTMClassification(X_train, Y_train, X_test, Y_test, metric, **args):\n",
    "    # Train and validate the network using LSTM\n",
    "    train_dataset = FPLSTMDataset(X_train, Y_train)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args['batch_size'], shuffle=True, collate_fn=FPLSTM_collate)\n",
    "    test_dataset = FPLSTMDataset(X_test, Y_test.values)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args['batch_size'], shuffle=True, collate_fn=FPLSTM_collate)\n",
    "    net_train_validate_LSTM(args['net'], args['optimizer'], train_loader, test_loader, args['epochs'], X_test.shape[0], Xtrain.shape[0], args['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification(X_train, Y_train, X_test, Y_test, classifier, metric, **args):\n",
    "\t\"\"\"\n",
    "\tPerform classification using the specified classifier.\n",
    "\n",
    "\tParameters:\n",
    "\t- X_train (array-like): Training data features.\n",
    "\t- Y_train (array-like): Training data labels.\n",
    "\t- X_test (array-like): Test data features.\n",
    "\t- Y_test (array-like): Test data labels.\n",
    "\t- classifier (str): The classifier to use. Options: 'RandomForest', 'TCN', 'LSTM'.\n",
    "\t- metric (str): The metric to evaluate the classification performance.\n",
    "\t- **args: Additional arguments specific to each classifier.\n",
    "\n",
    "\tReturns:\n",
    "\t- None\n",
    "\t\"\"\"\n",
    "\tprint('Classification using {} is starting'.format(classifier))\n",
    "\tif classifier == 'RandomForest':\n",
    "\t\trandomForestClassification(X_train, Y_train, X_test, Y_test, metric, **args)\n",
    "\telif classifier == 'TCN':\n",
    "\t\tTCNClassification(X_train, Y_train, X_test, Y_test, metric, **args)\n",
    "\telif classifier == 'LSTM':\n",
    "\t\tLSTMClassification(X_train, Y_train, X_test, Y_test, metric, **args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {\n",
    "    'Xiao_et_al': [\n",
    "        'date',\n",
    "        'serial_number',\n",
    "        'model',\n",
    "        'failure',\n",
    "        'smart_1_normalized', \n",
    "        'smart_5_normalized',\n",
    "        'smart_5_raw',\n",
    "        'smart_7_normalized',\n",
    "        'smart_9_raw',\n",
    "        'smart_12_raw',\n",
    "        'smart_183_raw',\n",
    "        'smart_184_normalized',\n",
    "        'smart_184_raw', \n",
    "        'smart_187_normalized',\n",
    "        'smart_187_raw',\n",
    "        'smart_189_normalized', \n",
    "        'smart_193_normalized',\n",
    "        'smart_193_raw',\n",
    "        'smart_197_normalized', \n",
    "        'smart_197_raw',\n",
    "        'smart_198_normalized',\n",
    "        'smart_198_raw',\n",
    "        'smart_199_raw'\n",
    "    ],\n",
    "    'iSTEP': [\n",
    "        'date',\n",
    "        'serial_number',\n",
    "        'model',\n",
    "        'failure',\n",
    "        'smart_5_raw',\n",
    "        'smart_3_raw', \n",
    "        'smart_10_raw',\n",
    "        'smart_12_raw',\n",
    "        'smart_4_raw',\n",
    "        'smart_194_raw', \n",
    "        'smart_1_raw',\n",
    "        'smart_9_raw',\n",
    "        'smart_192_raw',\n",
    "        'smart_193_raw', \n",
    "        'smart_197_raw',\n",
    "        'smart_198_raw',\n",
    "        'smart_199_raw'\n",
    "    ]\n",
    "}\n",
    "# many parameters that could be changed, both for unbalancing, for networks and for features.\n",
    "windowing = 1\n",
    "min_days_HDD = 115\n",
    "# TODO: Can be adjusted by dynamic parameters\n",
    "days_considered_as_failure = 7\n",
    "test_train_perc = 0.3\n",
    "# type of oversampling\n",
    "oversample_undersample = 2\n",
    "# balancing factor (major/minor = balancing_normal_failed)\n",
    "# TODO: We can calculate the imbalance ratio of the dataset and use this ratio to adjust the balancing factor.\n",
    "balancing_normal_failed = 20\n",
    "history_signal = 32\n",
    "# type of classifier\n",
    "classifier = 'LSTM'\n",
    "# if you extract features for RF for example. Not tested\n",
    "perform_features_extraction = False\n",
    "CUDA_DEV = \"0\"\n",
    "# if automatically select best features\n",
    "ranking = 'Ok'\n",
    "num_features = 18\n",
    "overlap = 1\n",
    "\n",
    "try:\n",
    "    df = pd.read_pickle(os.path.join('..', 'temp', f'{model}_Dataset_windowed_{history_signal}_rank_{ranking}_{num_features}_overlap_{overlap}.pkl'))\n",
    "except:\n",
    "    if ranking == 'None':\n",
    "        df = import_data(years=years, model=model, name='iSTEP', features=features)\n",
    "    else:\n",
    "        df = import_data(years=years, model=model, name='iSTEP')\n",
    "    print(df.head())\n",
    "    for column in list(df):\n",
    "        missing = round(df[column].notna().sum() / df.shape[0] * 100, 2)\n",
    "        print('{:.<27}{}%'.format(column, missing))\n",
    "    # drop bad HDs\n",
    "    bad_missing_hds, bad_power_hds, df = filter_HDs_out(df, min_days=min_days_HDD, time_window='30D', tolerance=30)\n",
    "    # predict_val represents the prediction value of the failure\n",
    "    # validate_val represents the validation value of the failure\n",
    "    df['predict_val'], df['validate_val'] = generate_failure_predictions(df, days=days_considered_as_failure, window=history_signal) # define RUL (remaining useful life) piecewise\n",
    "    if ranking != 'None':\n",
    "        df = feature_selection(df, num_features)\n",
    "    print('Used features')\n",
    "    for column in list(df):\n",
    "        print('{:.<27}'.format(column,))\t\n",
    "    ## -------- ##\n",
    "    # random: stratified without keeping timw\n",
    "    # hdd --> separate different hdd (need FIXes)\n",
    "    # temporal --> separate by time (need FIXes)\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = dataset_partitioning(\n",
    "    df,\n",
    "    model,\n",
    "    overlap=overlap,\n",
    "    rank=ranking,\n",
    "    num_features=num_features,\n",
    "    technique='random',\n",
    "    test_train_perc=test_train_perc,\n",
    "    windowing=windowing,\n",
    "    window_dim=history_signal,\n",
    "    resampler_balancing=balancing_normal_failed,\n",
    "    oversample_undersample=oversample_undersample\n",
    ")\n",
    "\n",
    "####### CLASSIFIER PARAMETERS #######\n",
    "if classifier == 'RandomForest':\n",
    "    pass\n",
    "elif classifier == 'TCN':\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = CUDA_DEV\n",
    "    batch_size = 256\n",
    "    lr = 0.001\n",
    "    num_inputs = Xtrain.shape[1]\n",
    "    net, optimizer = init_net(lr, history_signal, num_inputs)\n",
    "    epochs = 200\n",
    "elif classifier == 'LSTM':\n",
    "    lr = 0.001\n",
    "    batch_size = 256\n",
    "    epochs = 300\n",
    "    dropout = 0.1\n",
    "    #hidden state sizes (from [14])\n",
    "    # The dimensionality of the output space of the LSTM layer\n",
    "    lstm_hidden_s = 64\n",
    "    # The dimensionality of the output space of the first fully connected layer\n",
    "    fc1_hidden_s = 16\n",
    "    num_inputs = Xtrain.shape[1]\n",
    "    net = FPLSTM(lstm_hidden_s, fc1_hidden_s, num_inputs, 2, dropout)\n",
    "    net.cuda()\n",
    "    # We use the Adam optimizer, a method for Stochastic Optimization\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "## ---------------------------- ##\n",
    "\n",
    "if perform_features_extraction == True:\n",
    "    # Extract features for the train and test set\n",
    "    Xtrain = feature_extraction(Xtrain)\n",
    "    Xtest = feature_extraction(Xtest)\n",
    "\n",
    "if classifier == 'RandomForest' and windowing == 1:\n",
    "    Xtrain = Xtrain.reshape(Xtrain.shape[0], Xtrain.shape[1] * Xtrain.shape[2])\n",
    "    Xtest = Xtest.reshape(Xtest.shape[0], Xtest.shape[1] * Xtest.shape[2])\n",
    "\n",
    "try:\n",
    "    classification(\n",
    "        X_train=Xtrain,\n",
    "        Y_train=ytrain,\n",
    "        X_test=Xtest,\n",
    "        Y_test=ytest,\n",
    "        classifier=classifier,\n",
    "        metric=['RMSE', 'MAE', 'FDR', 'FAR', 'F1', 'recall', 'precision'],\n",
    "        net=net,\n",
    "        optimizer=optimizer,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        lr=lr\n",
    "    )\n",
    "except:\n",
    "    classification(\n",
    "        X_train=Xtrain,\n",
    "        Y_train=ytrain,\n",
    "        X_test=Xtest,\n",
    "        Y_test=ytest,\n",
    "        classifier=classifier,\n",
    "        metric=['RMSE', 'MAE', 'FDR', 'FAR', 'F1', 'recall', 'precision']\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
